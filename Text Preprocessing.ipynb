{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Using NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing is cleaning our data to obtain meaning information by removing all the unnecessary words, acronyms, smiles etc. \n",
    "\n",
    "We have demonstrated important things to consider while performing text preprocessing with the code.\n",
    "    \n",
    "    1. Converting Text to same Case\n",
    "    2. Tokenization\n",
    "    3. Stemming and Lemmatization\n",
    "    4. Stopwords\n",
    "    5. Normalization\n",
    "    6. Noise Word Removal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Same Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with text, we usually tend to have mix of both upper and lower case words which needs to be verified. Assume you have a sentence with \"India\", \"india\", \"INDIA\" in a text. In such the number of features will be 3 which should be 1. This increases unnecessary computation and not a good dataset. \n",
    "Thus, we make the text of same case-type.\n",
    "\n",
    "Normally, we convert the text in NLP in lowercase. But, the result obtain for the case is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his name is xyz. xyz is from abc. xyz studied in abc and has job in abc.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"His name is Xyz. XYZ is from ABC. xyz studied in abc and has job in Abc.\"\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When not reducing to lower case, with only two words we see 6 new words or 6 new features. The number can vary very highly if this case sensitivity is not checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting the text into words and sentences based on the requirement.\n",
    "The process is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "\"\"\"               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I|have|three|visions|for|India|.|In|3000|years|of|our|history|,|people|from|all|over|the|world|have|come|and|invaded|us|,|captured|our|lands|,|conquered|our|minds|.|From|Alexander|onwards|,|the|Greeks|,|the|Turks|,|the|Moguls|,|the|Portuguese|,|the|British|,|the|French|,|the|Dutch|,|all|of|them|came|and|looted|us|,|took|over|what|was|ours|.|Yet|we|have|not|done|this|to|any|other|nation|.|We|have|not|conquered|anyone|.|We|have|not|grabbed|their|land|,|their|culture|,|their|history|and|tried|to|enforce|our|way|of|life|on|them|.\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print('|'.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few points about tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that sentence ends with \".\" so we could have also used regular expression. \n",
    "\n",
    "But, for the case when \".\" is used in between sentences like \"Mr.\" or \"Dr.\" the sentence will spilt based on this. \n",
    "\n",
    "Thus we need to put mulitplt conditions. Thus we prefer nltk sent_tokenization which already have this inbuilt pattern checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "msg = \"Mr. XYS is the chairman of the company. He has two other companies also.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " ' XYS is the chairman of the company',\n",
       " ' He has two other companies also',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. XYS is the chairman of the company.', 'He has two other companies also.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Mr. XYS is the chairman of the company. He has two other companies also.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can use split function for the words, with word_tokenize we obtain faster result computationally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is to reduce word into stem words which may or may not have meaning.\n",
    "\n",
    "history --> histori | \n",
    "historical --> histor\n",
    "\n",
    "going --> go | \n",
    "goes --> goe | \n",
    "gone --> gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n",
      "histor\n",
      "\n",
      "go\n",
      "goe\n",
      "gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem(\"history\"))\n",
    "print(stemmer.stem(\"historical\"))\n",
    "print()\n",
    "print(stemmer.stem(\"going\"))\n",
    "print(stemmer.stem(\"goes\"))\n",
    "print(stemmer.stem(\"gone\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is to reduce into root word which definately contain meaning. For Lemmatization to work properly it require Part of Speech as an input. If the input is not provided, it assumes the word to be noun. Because this the computation speed decreases but better result are obtained.\n",
    "\n",
    "history --> histori |\n",
    "historical --> historical\n",
    "\n",
    "going --> go |\n",
    "went --> go |\n",
    "gone --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n",
      "historical\n",
      "\n",
      "go\n",
      "went\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"history\"))\n",
    "print(lemmatizer.lemmatize(\"historical\"))\n",
    "print()\n",
    "print(lemmatizer.lemmatize(\"going\", pos='v'))\n",
    "print(lemmatizer.lemmatize(\"went\"))\n",
    "print(lemmatizer.lemmatize(\"went\", pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming is faster than lemmatization\n",
    "\n",
    "most of the words after stemming does not generate meaningful words\n",
    "\n",
    "stemming may reduce two words with different root to same stem words \n",
    "\n",
    "stemming may reduce two words with same root word to different stem words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are the set of words that does not convey meaningful information. \n",
    "\n",
    "    Eg: the, is, am, are.\n",
    "\n",
    "So we can remove the stopwords and still able to generate a meaningful sentence. While nltk contains a set for stopwords, sometimes we have to add new words or remove few words from the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the list contain negation words which are very important when trying to understand negative words or sarcastic words.\n",
    "\n",
    "    Eg. I am not hungry --> I am hungry.\n",
    "    He is not dead --> He is dead\n",
    "    \n",
    "The meaning is very different here. So, sometimes we have to create our own stopwords set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He worked hard able succeed'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"He worked very hard but was not able to succeed\"\n",
    "text2 = [word for word in text1.split(' ') if word not in stop_words] \n",
    "text2 = ' '.join(text2)\n",
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure we infer that \"Hard provided positive result\". But, this is clearly not the case. So, we have to be careful while using stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When acronym of words are used it generally highlites importance of the word following it like Dr. , Mr. etc. \n",
    "But when we are having data from informal conversation often words are used like:\n",
    "    \n",
    "    2morrow - > tomorrow\n",
    "    2moro - > tomorrow\n",
    "    \n",
    "    :) - > smile\n",
    "    :-) - > smile\n",
    "    \n",
    "So, this conatins the same meaning and express the emotion of a person. So, the word smile and tomorrow and smile may be important but this different acronyms or symbols although maybe different, convey the same meaning. So, depending on the task it may be of hugh significance like in customer review. \n",
    "\n",
    "This words neither comes under the category of stopwords nor a part of english language. Thus, we convert into english language. This can be done collecting all the words that does not have only letters and than deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2morrow.', 'Evr1', 'b4', '8.', ':)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"We will meet 2morrow. Evr1 must come b4 8. :)\"\n",
    "words = [word for word in text.split(\" \") if word.isalpha()==False]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generally a higly overlooked but very important part of text preprocessing while working on sentiment analysis. Although it is time consuming but significant improvement are obtained when normalization is done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with informal text like tweets, we come across Scenario likes\n",
    "\n",
    "    ..trouble..\n",
    "    trouble<\n",
    "    1.trouble\n",
    "    \n",
    "Performing stemming on this words does not change such words to stem word nor lemmatization works. So, in such case we have to remove unnecessary \".\" or performing cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  trouble  trouble    trouble'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"..trouble. trouble< 1.trouble\"\n",
    "message = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we replaced every character except english alphabet with space removing space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
